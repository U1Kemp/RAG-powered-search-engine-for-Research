from keybert import KeyBERT
from keyphrase_vectorizers import KeyphraseCountVectorizer
import uuid
import arxiv
import wikipedia
import json
import asyncio
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from sentence_transformers import SentenceTransformer
from qdrant_client import async_qdrant_client
from qdrant_client.http import models
import fitz  # PyMuPDF
import io

# client = QdrantClient(":memory:")
COLLECTION_PREFIX = "rag_session_"

client = async_qdrant_client.AsyncQdrantClient(":memory:")

# set up qdrant
COLLECTION_PREFIX = "rag_session_"
session_id = 'test'

embedder = SentenceTransformer("all-MiniLM-L6-v2")  # Sentence embeddings

# Load the model and tokenizer for summary generation
model_name = "sshleifer/distilbart-cnn-12-6"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

def summarize(text: str, max_input_tokens:int = 1024, max_output_tokens:int =512) -> str:
    """
    This function takes a text as input and returns a summary of it. The summary is
    generated by a BART model, which is a type of transformer model that is designed
    for text summarization.

    Parameters
    ----------
    text : str
        The input text to be summarized.
    max_input_tokens : int
        The maximum number of tokens that the model can accept as input. If the input
        text is longer than this, it will be truncated.
    max_output_tokens : int
        The maximum number of tokens that the model can generate as output. If the
        generated summary is longer than this, it will be truncated.

    Returns
    -------
    str
        The summary of the input text.
    """
    if not isinstance(text, str):
        raise TypeError("Input text must be a string.")
    
    # Tokenize input (truncate if it's too long)
    inputs = tokenizer(
        text,
        return_tensors="pt",
        max_length=max_input_tokens,
        truncation=True,
        padding="longest"
    )

    # Generate summary
    summary_ids = model.generate(
        inputs["input_ids"],
        num_beams=4,
        max_length=max_output_tokens,
        early_stopping=True
    )

    # Decode and return the summary
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

def remove_duplicate_dicts(list_of_dicts):
    """
    This function takes a list of dictionaries and removes any duplicates, based on
    the contents of the dictionaries. It returns a new list with the duplicates removed.

    The function works by converting each dictionary to a JSON string, and then storing
    these strings in a set. Since sets only store unique values, any duplicate strings
    will be ignored. The original dictionaries are then added to a new list, which is
    returned as the result.

    Parameters
    ----------
    list_of_dicts : list
        A list of dictionaries.

    Returns
    -------
    list
        A new list with the duplicates removed.
    """
    if not isinstance(list_of_dicts, list):
        raise TypeError("list_of_dicts must be a list")
    if not all(isinstance(d, dict) for d in list_of_dicts):
        raise TypeError("All elements in list_of_dicts must be dictionaries")
    
    seen = set()
    result = []
    for d in list_of_dicts:
        frozen = json.dumps(d, sort_keys=True)  # Sort keys for consistent hashing
        if frozen not in seen:
            seen.add(frozen)
            result.append(d)
    return result

def extract_keywords(query: str, top_n: int = 5, threshold: float = 0.75) -> list:
    """
    This function takes a query string and returns a list of keywords extracted from
    the query. The number of keywords returned can be specified with the top_n
    parameter. The relevance threshold for inclusion in the output can be set with the
    threshold parameter.

    Parameters
    ----------
    query : str
        The query from which to extract keywords.
    top_n : int, optional
        The maximum number of keywords to return. Defaults to 5.
    threshold : float, optional
        Keywords with a relevance score above this threshold will be included in the
        output. Defaults to 0.5.

    Returns
    -------
    list
        A list of keywords extracted from the query.
    """
    # Check if query is a string
    if not isinstance(query, str):
        raise TypeError("Query must be a string")

    # Check if top_n is an integer
    if not isinstance(top_n, int):
        raise TypeError("top_n must be an integer")

    # Check if threshold is a float
    if not isinstance(threshold, float):
        raise TypeError("threshold must be a float")

    # Check if threshold is between 0 and 1
    if threshold > 1 or threshold < 0:
        raise ValueError("threshold must be a float in [0,1]")

    # Create a KeyBERT model
    kw_model = KeyBERT()

    # Create a CountVectorizer for keyphrases
    vectorizer = KeyphraseCountVectorizer()

    # Extract keywords with the KeyBERT model
    try:
        keywords = kw_model.extract_keywords(
            query, keyphrase_ngram_range=(1, 5), vectorizer=vectorizer, top_n=top_n
        )
    except:
        return []

    # Filter the keywords based on the relevance threshold
    search_words = [word[0] for word in keywords if word[1] >= threshold]

    # Return the list of search words
    return search_words

with open('sub2tag.json', 'r') as f:
    sub2tag = json.load(f)

with open('tag2sub.json', 'r') as f:
    tag2sub = json.load(f)

def get_sub(tag):
    """
    Join the subject and subtopic for a given tag.
    """
    return ' '.join(tag2sub[tag])

def get_arxiv_paper_sync(subject: str = "", subtopic:str = "", query: str = "", max_results: int = 5, priority: str = 'relevance') -> list[dict]:
    """
    Fetches a list of research papers from arXiv based on the given query.

    Parameters
    ----------
    query : str
        The search query to find relevant papers.
    max_results : int
        The maximum number of papers to retrieve. Defaults to 5.
    priority : str
        The sorting criterion for the results. Options are 'relevance', 'submitted', and 'updated'. Defaults to 'relevance'.

    Returns
    -------
    list[dict]
        A list of dictionaries containing paper metadata: id, title, text, and source.
    """
    if not isinstance(query, str):
        raise TypeError("Query must be a string")
    
    # Map the priority string to an arXiv sort criterion
    sort_by = {
        'relevance': arxiv.SortCriterion.Relevance,
        'submitted': arxiv.SortCriterion.SubmittedDate,
        'updated': arxiv.SortCriterion.LastUpdatedDate
    }.get(priority, arxiv.SortCriterion.Relevance)

    # Create a search object for the arXiv query
    if subject!="" and subtopic!="":
        search_query = f"cat:{sub2tag[subject][subtopic]} AND abs:{query}"
    elif subject!="" and subtopic=="":
        search_query = f"cat:{sub2tag[subject]} AND abs:{query}"
    else:
        search_query = query
    search = arxiv.Search(query=search_query, max_results=max_results, sort_by=sort_by)
    client = arxiv.Client()

    papers = []

    # Iterate over search results and extract relevant information
    for result in client.results(search):
        paper_id = f"arxiv_{result.entry_id.split('/')[-1]}"
        paper_text = f"{result.title}\n\n{result.summary}"
        papers.append({
            "id": paper_id,
            "title": result.title,
            "text": paper_text,
            "source": result.entry_id
        })

    return papers

# Async wrapper using asyncio.to_thread
async def get_arxiv_paper(subject:str, subtopic:str, query: str, max_results: int = 5, priority: str = 'relevance') -> list[dict]:
    return await asyncio.to_thread(get_arxiv_paper_sync, subject, subtopic, query, max_results, priority)

# Main async fetcher
async def fetch_arxiv_papers(subject:str, subtopic:str,
    queries: list[str], max_results: int = 5, priority: str = 'relevance'
) -> list[dict]:
    """
    Fetches metadata for recent papers from arXiv based on a list of queries.

    Parameters
    ----------
    queries : list[str]
        A list of queries to search for.
    max_results : int, optional
        The maximum number of papers to return for each query. Defaults to 5.
    priority : str, optional
        The sorting criteria for the results. Options are 'relevance', 'submitted',
        'updated', and 'random'. Defaults to 'relevance'.

    Returns
    -------
    list[dict]
        A list of dictionaries with the paper id, title, text, and source.
    """
    tasks = [
        # For each query, get the arXiv papers
        get_arxiv_paper(subject, subtopic, query, max_results, priority)
        for query in queries
    ]
    papers_nested = await asyncio.gather(*tasks)
    # Flatten the list of lists into a single list
    return [paper for sublist in papers_nested for paper in sublist]

def chunk_text(text: str, chunk_size: int = 512, overlap: int = 64) -> list[str]:
    """
    Splits text into overlapping chunks. This is useful for storing text in a
    database or other storage system that has size limitations.

    Parameters
    ----------
    text : str
        The text to split into chunks.
    chunk_size : int, optional
        The size of each chunk. Defaults to 256.
    overlap : int, optional
        The amount of overlap between chunks. Defaults to 32.

    Returns
    -------
    list[str]
        A list of strings, where each string is a chunk of the input text.
    """
    if not isinstance(text, str):
        raise TypeError("Text must be a string")
    
    words = text.split()
    chunks = []
    start = 0
    while start < len(words):
        # Get the words for the current chunk
        chunk = words[start : start + chunk_size]
        # Join the words with spaces to form a string
        chunks.append(" ".join(chunk))
        # Shift the start to ensure the next chunk has the specified overlap
        start += chunk_size - overlap
    return chunks

def get_wiki_page_sync(
    query: str, max_sections: int = 15, num_results: int = 5, chunk_size: int = 128, overlap: int = 0
) -> list[dict]:
    """
    Fetches relevant content from Wikipedia and stores it in a list of dictionaries.

    Parameters
    ----------
    query : str
        The search query to look for on Wikipedia.
    max_sections : int, optional
        The maximum number of sections (paragraphs) to fetch from each Wikipedia page. Defaults to 15.
    num_results : int, optional
        The number of search results to consider. Defaults to 5.
    chunk_size : int, optional
        The size of each text chunk. Defaults to 128.
    overlap : int, optional
        The number of words that overlap between chunks. Defaults to 0.

    Returns
    -------
    list[dict]
        A list of dictionaries, each containing an ID, title, text chunk, and source URL for sections of Wikipedia content.
    """
    if not isinstance(query, str):
        raise TypeError("Query must be a string")
    
    results = wikipedia.search(query, results=num_results)
    wiki_content = []
    for result in results:
        try:
            # Retrieve the content of the Wikipedia page
            page_content = wikipedia.page(result).content
            # Split content into sections (paragraphs)
            text_sections = page_content.split("\n\n")[:max_sections]
            # Process each section
            for idx, section_text in enumerate(text_sections):
                if section_text.strip():
                    # Chunk the section text
                    chunks = chunk_text(section_text.strip(), chunk_size, overlap)
                    # Store each chunk with metadata
                    for chunk_idx, chunk in enumerate(chunks):
                        wiki_content.append({
                            "id": f"wiki_{uuid.uuid4().hex[:8]}",
                            "title": f"{result} - Section {idx+1} - Chunk {chunk_idx+1}",
                            "text": chunk,
                            "source": f"https://en.wikipedia.org/wiki/{result.replace(' ', '_')}"
                        })
        except (wikipedia.exceptions.PageError, wikipedia.exceptions.DisambiguationError):
            # Skip pages that cannot be retrieved
            pass
    return wiki_content

# Async wrapper using asyncio.to_thread
async def get_wiki_page(query: str, max_sections: int = 15, num_results: int = 5, chunk_size: int = 512, overlap: int = 64):
    return await asyncio.to_thread(get_wiki_page_sync, query, max_sections, num_results, chunk_size, overlap)

        
# Main async fetcher
async def fetch_wikipedia_content(
    queries: list[str], max_sections: int = 15, num_results: int = 5, chunk_size: int = 512, overlap: int = 64
) -> list[dict]:
    """
    Fetches relevant content from Wikipedia, given a list of search queries.

    Parameters
    ----------
    queries : list[str]
        A list of search queries to look for on Wikipedia.
    max_sections : int, optional
        The maximum number of sections (paragraphs) to fetch from each Wikipedia page. Defaults to 15.
    num_results : int, optional
        The number of search results to consider. Defaults to 5.
    chunk_size : int, optional
        The size of each text chunk. Defaults to 180.
    overlap : int, optional
        The number of words that overlap between chunks. Defaults to 0.

    Returns
    -------
    list[dict]
        A list of dictionaries, each containing an ID, title, text chunk, and source URL for sections of Wikipedia content.
    """
    if not isinstance(queries, list):
        raise TypeError("Queries must be a list of strings")
    
    # if list is empty, return empty list 
    if queries==[]:
        return []
    
    tasks = [
        # For each query, get the relevant Wikipedia content
        get_wiki_page(query, max_sections, num_results, chunk_size, overlap)
        for query in queries
    ]
    wiki_content_nested = await asyncio.gather(*tasks)
    # Flatten the list of lists into a single list
    return [chunk for sublist in wiki_content_nested for chunk in sublist]

async def process_pdf_file(file_obj):
    """
    Process a PDF file and extract its text content.
    
    Args:
        file_obj: File object from Flask request
        
    Returns:
        str: Extracted text from the PDF
    """
    # Save the file to a temporary in-memory buffer
    pdf_data = file_obj.read()
    pdf_stream = io.BytesIO(pdf_data)
    
    # Open the PDF with PyMuPDF
    doc = fitz.open(stream=pdf_stream, filetype="pdf")
    
    # Extract text from each page
    text = ""
    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        text += page.get_text()
        
    return text


async def store_content(COLLECTION_PREFIX: str, session_id: str, documents: list[dict], batch_size: int = 128):
    """ 
    Stores documents in Qdrant after encoding them into embeddings, using batches for efficiency.

    Args:
        session_id: The ID of the session for which documents are being stored.
        documents: A list of dictionaries, where each dictionary contains "text", "title", and "source" keys.
        batch_size: The number of documents to process and upsert into Qdrant in each batch.
    """
    if not isinstance(documents, list):
        raise TypeError("Documents must be a list of dictionaries")

    if not all(isinstance(doc, dict) and "text" in doc and "title" in doc and "source" in doc for doc in documents):
        raise ValueError("Each document must contain 'text', 'title', and 'source' keys")
    
    if not isinstance(batch_size, int) or batch_size <= 0:
        raise ValueError("Batch size must be a positive integer")

    collection_name = COLLECTION_PREFIX + session_id

    # check if collection exists
    try:
        await client.get_collection(collection_name=collection_name)
    except:
        await client.create_collection(
            collection_name=collection_name,
            vectors_config=models.VectorParams(size=384, distance=models.Distance.COSINE, on_disk=False),
        )

    # Encode the document text into a vector
    embedding = embedder.encode([doc["text"] for doc in documents], batch_size=batch_size, show_progress_bar=True)

    # Prepare the data structure for Qdrant
    points = models.Batch(
    ids=[uuid.uuid4().hex for _ in range(len(documents))],
    vectors=embedding,
    payloads=[{"title": doc["title"], "text": doc["text"], "source": doc["source"]} for doc in documents]
)

    # Upsert the batch into Qdrant when batch size is met or at the end
    try:
        await client.upsert(collection_name=collection_name, points=points, wait=True)
    except Exception as e:
        print(f"Error upserting batch: {e}")
    

async def retrieve_content(COLLECTION_PREFIX: str, session_id: str, query: str, top_k: int=10, threshold: float=0.5) -> list[dict]:
    """
    Retrieves content from Qdrant for a given query.

    Args:
        session_id: The ID of the session for which content is being retrieved.
        query: The query to search for.
        top_k: The number of top results to retrieve. Defaults to 5.
        threshold: The similarity threshold for filtering results. Defaults to 0.5.

    Returns:
        A list of dictionaries, where each dictionary contains "text", "title", and "source" keys.  
    """
    if not isinstance(top_k, int) or top_k <= 0:
        raise ValueError("top_k must be a positive integer")
    
    if not isinstance(threshold, float) or threshold < 0 or threshold > 1:
        raise ValueError("threshold must be a float between 0 and 1")
    
    if not isinstance(query, str):
        raise TypeError("Query must be a string")
    
    collection_name = COLLECTION_PREFIX + session_id
    # Encode the query into a vector
    query_embedding = embedder.encode(query).tolist()

    # Search Qdrant for the top-k documents with cosine similarity above the threshold
    search_results = await client.search(
        collection_name=collection_name,
        query_vector=query_embedding,
        limit=top_k,
        score_threshold=threshold
    )

    # Return the top-k results as a list of dictionaries containing text, title, and source
    return [{"text": result.payload["text"], "title": result.payload["title"], "source": result.payload["source"]} for result in search_results]

async def delete_collection(COLLECTION_PREFIX: str, session_id: str):
    """
    Deletes the Qdrant collection for the given session ID.

    Args:
        session_id: The ID of the session for which the collection is being deleted.
    """
    collection_name = COLLECTION_PREFIX + session_id
    # Delete the collection
    await client.delete_collection(collection_name=collection_name)
    # Delete all points
    # from qdrant_client.http.models import Filter
    # await client.delete(
    #     collection_name=collection_name,
    #     points_selector=Filter(must=[]),  # An empty filter matches all points
    #     wait=True  # Optional: waits until deletion is applied
    # )
    print(f"Collection {collection_name} deleted.")
